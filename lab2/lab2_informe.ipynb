{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2 - Informe\n",
    "\n",
    "### Grupo 4:\n",
    "     - S. Calvo C.I 5.711.417-7     \n",
    "     - X. Iribarnegaray C.I 5.253.705-9\n",
    "     - J. Simonelli C.I 5.405.358-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivos\n",
    "El objetivo de este laboratorio es:\n",
    "- Implementar el algoritmo Naive Bayes\n",
    "- Aplicar herramientas de metodología\n",
    "- Analizar los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diseño\n",
    "### 2.1 Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicar Naive Bayes logaritmico, y presentar el otro analogo ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from naive_bayes import init\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "def naive_bayes(dataset, target, features, instance, m):\n",
    "    dataset_size = dataset.shape[0]\n",
    "    prob_1 = dataset[target].value_counts()[1]/dataset_size\n",
    "    prob_0 = dataset[target].value_counts()[0]/dataset_size\n",
    "    \n",
    "    sum_1 = np.log(prob_1)\n",
    "    sum_0 = np.log(prob_0)\n",
    "    \n",
    "    for feature in features:\n",
    "        examples = dataset.loc[dataset[feature] == instance[feature]][target].value_counts()\n",
    "        \n",
    "        # if no instances with a specific target value is found, the get method will return 0\n",
    "        count_1 = examples.get(1, default=0)\n",
    "        count_0 = examples.get(0, default=0)\n",
    "        \n",
    "        feature_range = len(dataset[feature].value_counts())\n",
    "        \n",
    "        numerator_1 = count_1 + (m / feature_range)\n",
    "        numerator_0 = count_0 + (m / feature_range)\n",
    "\n",
    "        # sum of sequence\n",
    "        sum_1 += np.log( numerator_1 / (dataset[target].value_counts()[1] + m) )\n",
    "        sum_0 += np.log( numerator_0 / (dataset[target].value_counts()[0] + m) )\n",
    "        \n",
    "    # argmax\n",
    "    if ( sum_1 > sum_0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def test_instances(X_train, y_train, X_test, y_test, features, m):\n",
    "    y_pred = []\n",
    "    train_ds = X_train.copy()\n",
    "    train_ds[target] = y_train\n",
    "    for i in range(0, X_test.shape[0]):\n",
    "        instance = X_test.iloc[i]\n",
    "        y_pred.append(naive_bayes(train_ds, target, features, instance, m))\n",
    "        \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=None, zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average=None, zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred, average=None, zero_division=1)\n",
    "    \n",
    "    return accuracy*100, precision*100, recall*100, f1*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 1, Accuracy = 0.8551401869158879\n",
      "\n",
      "   Precision    Recall        F1\n",
      "0   0.878338  0.933754  0.905199\n",
      "1   0.769231  0.630631  0.693069\n",
      "\n",
      "Confusion Matrix:\n",
      "[[296  21]\n",
      " [ 41  70]]\n",
      "\n",
      "\n",
      "m = 10, Accuracy = 0.8598130841121495\n",
      "\n",
      "   Precision    Recall        F1\n",
      "0   0.885886  0.930599  0.907692\n",
      "1   0.768421  0.657658  0.708738\n",
      "\n",
      "Confusion Matrix:\n",
      "[[295  22]\n",
      " [ 38  73]]\n",
      "\n",
      "\n",
      "m = 100, Accuracy = 0.8411214953271028\n",
      "\n",
      "   Precision    Recall        F1\n",
      "0   0.867257  0.927445  0.896341\n",
      "1   0.741573  0.594595  0.660000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[294  23]\n",
      " [ 45  66]]\n",
      "\n",
      "\n",
      "m = 1000, Accuracy = 0.7453271028037384\n",
      "\n",
      "   Precision    Recall        F1\n",
      "0   0.745283  0.996845  0.852901\n",
      "1   0.750000  0.027027  0.052174\n",
      "\n",
      "Confusion Matrix:\n",
      "[[316   1]\n",
      " [108   3]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize KBinsDiscretizer for continuous features\n",
    "kbins = KBinsDiscretizer(n_bins=50, encode='ordinal', strategy='kmeans')\n",
    "\n",
    "# Discretize continuous features\n",
    "dataset[continuous_features] = kbins.fit_transform(dataset[continuous_features])\n",
    "\n",
    "# Prepare X and y\n",
    "X = dataset.drop([target, 'pidnum'], axis=1)\n",
    "y = dataset[target]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# List of m values\n",
    "m_values = [1, 10, 100, 1000]\n",
    "\n",
    "\n",
    "# Iterate over m values\n",
    "for i in range(0, len(m_values)):\n",
    "    results = pd.DataFrame(columns=['Precision', 'Recall', 'F1'])\n",
    "    \n",
    "    m = m_values[i]\n",
    "    \n",
    "    y_pred = []\n",
    "    \n",
    "    # Copy the training set and append target variable to it\n",
    "    train_ds = X_train.copy()\n",
    "    train_ds[target] = y_train\n",
    "    \n",
    "    # For each instance in the test set, make a prediction\n",
    "    for j in range(X_test.shape[0]):\n",
    "        instance = X_test.iloc[j]\n",
    "        y_pred.append(naive_bayes(train_ds, target, features, instance, m))\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=None)\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    results.loc[0] = {'Precision': precision[0], 'Recall': recall[0], 'F1': f1[0]}\n",
    "    results.loc[1] = {'Precision': precision[1], 'Recall': recall[1], 'F1': f1[1]}\n",
    "    \n",
    "    print(f\"m = {m}, Accuracy = {accuracy}\\n\")\n",
    "    print(f\"{results}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      k  Accuracy  Precision    Recall        F1\n",
      "0   5.0  0.845243   0.841391  0.845243  0.842614\n",
      "1  10.0  0.848975   0.845148  0.848975  0.846223\n",
      "2  15.0  0.847574   0.845527  0.847574  0.846119\n",
      "3  20.0  0.854585   0.851199  0.854585  0.852014\n",
      "4  23.0  0.849910   0.846376  0.849910  0.847258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "class CustomNaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features, m):\n",
    "        self.features = features\n",
    "        self.m = m\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes_ = np.unique(y_train)\n",
    "        self.X_train = X_train.copy()\n",
    "        self.y_train = y_train.copy()\n",
    "        self.X_train[target] = y_train\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for i in range(0, X_test.shape[0]):\n",
    "            instance = X_test.iloc[i]\n",
    "            y_pred.append(naive_bayes(self.X_train, target, self.features, instance, self.m))\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    \n",
    "    def __sklearn_clone__(self):\n",
    "        return self\n",
    "\n",
    "results = pd.DataFrame(columns=['k', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "k_range = [5, 10, 15, 20, 23]\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',                             # Accuracy score\n",
    "    'precision_micro': make_scorer(precision_score, average='micro'),  # micro precision\n",
    "    'recall_micro': make_scorer(recall_score, average='micro'),        # micro recall\n",
    "    'f1_micro': make_scorer(f1_score, average='micro')                 # micro F1 score\n",
    "}\n",
    "\n",
    "for i in k_range:\n",
    "    selector = SelectKBest(chi2, k=i)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    selected_features = X_train.columns[selector.get_support()]\n",
    "    model = CustomNaiveBayes(selected_features, m=10)\n",
    "    X_selected = X[selected_features]\n",
    "\n",
    "    scores = cross_validate(model, X_selected, y, cv=5, scoring=['accuracy', 'precision_weighted', 'recall_weighted','f1_weighted'] )\n",
    "    \n",
    "    results.loc[results.shape[0]] = [i, scores['test_accuracy'].mean(), scores['test_precision_weighted'].mean(), scores['test_recall_weighted'].mean(), scores['test_f1_weighted'].mean()]\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, model.predict_y(X_selected))}\\n\\n\")\n",
    "print(results)\n",
    "    \n",
    "    # # Generate a sequence of x values corresponding to the number of features selected\n",
    "    # x_values = range(1, 24)  # From 1 to 23 features\n",
    "\n",
    "    # # Plot the accuracy values\n",
    "    # plt.plot(x_values, acc, marker='.', linestyle='-', color='b')\n",
    "\n",
    "    # # Add labels and title\n",
    "    # plt.xlabel('Number of Selected Features')\n",
    "    # plt.ylabel('Cross-Validated F1 Score')\n",
    "    # plt.title('F1 Score vs. Number of Selected Features')\n",
    "\n",
    "    # # Display the plot\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
