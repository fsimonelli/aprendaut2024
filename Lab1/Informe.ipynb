{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 1 - Informe\n",
    "\n",
    "### Grupo 4:\n",
    "     - S. Calvo C.I 5.711.417-7     \n",
    "     - X. Iribarnegaray C.I 5.253.705-9\n",
    "     - J. Simonelli C.I 5.405.358-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este laboratorio es:\n",
    "- Implementar el algoritmo ID3, añadiendo el hiperparámetro *max_range_split*, que determina la cantidad máxima de rangos en los que se puede partir un atributo númerico.\n",
    "- Utilizar scikit-learn para el preprocesamiento de datos y la creación de modelos basados en árboles de decisión.\n",
    "- Evaluar y comparar los modelos generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diseño\n",
    "### 2.1 Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(dataset, target, features, continuous_features, max_range_splits, intact_dataset):\n",
    "    if len(features) == 0 or len(dataset[target].value_counts().index) == 1:\n",
    "        # value_counts[0] is either the only or the most common target value left in the current dataset.\n",
    "        return dataset[target].value_counts().index[0] \n",
    " \n",
    "    best, best_splits = best_feature(dataset, target, features, continuous_features, max_range_splits)\n",
    "    decision_tree = {best: {}}\n",
    "    \n",
    "    new_features = features.copy()\n",
    "    new_features.remove(best)\n",
    "    \n",
    "    original_dataset = intact_dataset\n",
    "    \n",
    "    if best_splits:\n",
    "        original_dataset = split_dataset(intact_dataset, best, best_splits)\n",
    "        dataset = split_dataset(dataset, best, best_splits)\n",
    "        \n",
    "    for value in original_dataset[best].value_counts().index:\n",
    "        examples = dataset.loc[dataset[best] == value]\n",
    "        if (len(examples) == 0):\n",
    "            decision_tree[best][value] = original_dataset.loc[original_dataset[best] == value][target].value_counts().index[0]\n",
    "        else:\n",
    "            decision_tree[best][value] = id3(examples, target, new_features, continuous_features, max_range_splits, intact_dataset)\n",
    "    \n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo ID3 es un algoritmo recursivo que, dado un dataset, construye un árbol de decisión de forma recursiva. A continunación describiremos nuestra implementación del algoritmo.\n",
    "\n",
    "#### Casos base:\n",
    "- Si no quedan features (atributos) a analizar, etiquetar la hoja con el valor más común en el dataset restante.\n",
    "- Si todos los ejemplos restantes en el dataset tienen el mismo valor target, etiquetar la hoja con ese valor. \n",
    "\n",
    "En este caso, para ambas condiciones clasificamos la hoja con el valor `dataset[target].value_counts().index[0]`. La función `value_counts()` retorna los valores de la columna target en el dataset, ordenados de más a menos común. Al tomar el primero de estos valores, nos aseguramos de etiquetar la hoja con el más común, para el primer caso, y de etiquetar la hoja con el único valor restante en el dataset para el segundo.\n",
    "\n",
    "#### Recursión:\n",
    "Si no se cumplen las condiciones de los casos base, entonces, el siguiente paso es elegir el mejor atributo: en el caso de nuestra implementación, el atributo que maximiza la ganancia (def. en Teorico). Este se obtiene mediante `best_feature`, que retorna tanto el mejor atributo como, en el caso de que este sea continuo, el mejor o mejores puntos de corte (`best_splits`). Si el atributo es contniuo, es decir, `best_splits` está definido, discretizamos utilizando estos puntos de corte los valores del mejor atributo, tanto en el conjunto de ejemplos restantes como en una copia del dataset_original.\n",
    "\n",
    "Luego, por cada valor posible que puede tomar el mejor atributo (en nuestro caso, los valores que toma este atributo en el dataset original o en la copia discretizada que obtuvimos en el paso anterior), construimos una rama del árbol. En estas ramas, si no hay ejemplos con el mismo valor para el atributo en cuestión, se asigna el valor objetivo más común de los ejemplos del conjunto de datos original, cuyo valor de atributo sea el mismo que el que está siendo evaluado en el momento. De lo contrario, se llama de forma recursiva a la función id3, quitando este atributo del conjunto de atributos a evaluar y usando como nuevo dataset los ejemplos del mismo valor de atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature(dataset, target, features, continuous_features, max_range_splits):\n",
    "    conditional_entropies = []\n",
    "    continuous = {}\n",
    "    for feature in features:\n",
    "        # Continuous-Valued feature \n",
    "        if feature in continuous_features:\n",
    "            aux_conditional_entropy, best_split = get_splits(dataset, feature, target, max_range_splits)\n",
    "            conditional_entropies.append(aux_conditional_entropy)\n",
    "            continuous[feature] = best_split\n",
    "        else :\n",
    "            res = 0\n",
    "            for value, count in dataset[feature].value_counts().items():\n",
    "                res += count*entropy(dataset.loc[dataset[feature] == value], target)\n",
    "            conditional_entropies.append(res / dataset.shape[0])\n",
    "    best_feature = features[conditional_entropies.index(min(conditional_entropies))]\n",
    "    \n",
    "    if not (best_feature in continuous):\n",
    "        return best_feature, None\n",
    "    return best_feature, continuous[best_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función best_attribute se encarga de calcular, dado un conjunto de ejemplos, el atributo que maximiza la ganancia de información, como se definió en el marco teórico. De manera equivalente, también puede decirse que selecciona el atributo que minimiza el sustraendo en la fórmula de la ganancia.\\\n",
    "\\\n",
    "Si el atributo a evaluar es continuo, los posibles puntos de corte para categorizar los valores de este atributo en rangos se obtienen mediante la función `get_splits(dataset, feature, target, max_range_splits)`. Esta función realiza lo siguiente:\n",
    "- Ordena de menor a mayor de valores: Ordena los valores del atributo en el conjunto de ejemplos de menor a mayor.\n",
    "- Identifica los puntos de corte: Recorre la columna target en ese orden, y cuando el valor de target cambia (por ejemplo, entre las filas i e i+1), se registra el promedio entre el valor del atributo en la fila i y el valor en la fila i+1 como un posible punto de corte.\\\n",
    "\n",
    "Para reducir el tiempo de ejecución del algoritmo, si la cantidad de posibles puntos de corte supera los 50, se seleccionan 50 puntos de corte al azar. Luego, se generan todas las combinaciones posibles de estos puntos, ordenadas en tamaños de hasta max_range_split - 1.\\\n",
    "\\\n",
    "Si el número de posibles puntos de corte es menor o igual a 50, procedemos por calcular directamente el valor del sustraendo mencionado previamente.\\\n",
    "\\\n",
    "El objetivo de este proceso es dividir los valores continuos en rangos mediante puntos de corte que se calculan como el promedio de los valores del atributo para dos ejemplos consecutivos con diferentes valores en el target. Dependiendo del valor del hiperparámetro max_range_splits (que puede tomar valores de 2 o 3), utilizamos combinaciones de estos puntos para crear los rangos. Finalmente, se obtiene un atributo de valores discretos.\\\n",
    "\\\n",
    "Es importante considerar que, en datasets de gran tamaño, es probable que existan muchos posibles puntos de corte. Por ello, es necesario identificar el mejor o los mejores puntos de corte. De manera similar al cálculo del \"mejor atributo\", se evalúa la ganancia de información para cada posible split, seleccionando aquel que minimice el segundo factor de la fórmula de la ganancia. Para este propósito, se utiliza la función get_splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(dataset, feature, target, max_range_splits):\n",
    "    min_conditional_entropy = 2\n",
    "    dataset = dataset.sort_values(by=feature)\n",
    "    current_target = dataset[target].iloc[0]\n",
    "    dataset_size = dataset.shape[0]\n",
    "    candidate_splits = []\n",
    "    best_splits = []\n",
    "    \n",
    "    # Finding splits\n",
    "    for i in range(1, dataset_size):\n",
    "        if current_target != dataset[target].iloc[i]:\n",
    "            candidate_splits.append((dataset[feature].iloc[i-1] + dataset[feature].iloc[i])/2)\n",
    "            current_target = dataset[target].iloc[i]\n",
    "    \n",
    "    sample = candidate_splits\n",
    "    if len(candidate_splits) > 50:\n",
    "        sample = random.sample(candidate_splits, 50)\n",
    "    \n",
    "    splits = generate_combinations(sample, max_range_splits)\n",
    " \n",
    "    for split in splits:\n",
    "        splitted_dataset = split_dataset(dataset, feature, split)\n",
    "        aux_conditional_entropy = 0\n",
    "        for value, count in splitted_dataset[feature].value_counts().items():\n",
    "            aux_conditional_entropy += count*entropy(splitted_dataset.loc[splitted_dataset[feature] == value], target)\n",
    "        aux_conditional_entropy = aux_conditional_entropy / splitted_dataset.shape[0]\n",
    "            \n",
    "        if (aux_conditional_entropy < min_conditional_entropy):\n",
    "            min_conditional_entropy = aux_conditional_entropy\n",
    "            best_splits = split\n",
    "            \n",
    "    return (min_conditional_entropy, best_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(dataset, target):\n",
    "    # value_counts() returns a Series containing the counts of unique values\n",
    "    values = dataset[target].value_counts()\n",
    "    # shape returns the size of dataset, shape[0] being the number of rows\n",
    "    total = dataset.shape[0]\n",
    "    p0 = values.iloc[0]/total\n",
    "    if (len(values) > 1):\n",
    "        p1 = values.iloc[1]/total\n",
    "        return -(p0)*np.log2(p0) - (p1) * np.log2(p1)\n",
    "    else: \n",
    "        return -(p0)*np.log2(p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Evaluación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de la siguiente sección de **Experimentación**, dividiremos el dataset original en conjuntos de entrenamiento y de prueba, en diferentes proporciones para experimentar con el sobreajuste del modelo. Más allá de esto, para la prueba de nuestro algoritmo y la comparación entre los clasificadores de scikit-learn con nuestra implementación de ID3 fijaremos la división en 80/20 train-test.\\\n",
    "\\\n",
    "Para medir la correctitud de nuestra implementación, utilizaremos la métrica de *accuracy* al asignar valores objetivo a nuevos ejemplos de instancias, y la calcularemos con la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_instances(tree, dataset):\n",
    "    res = 0\n",
    "    for i in range(0,dataset.shape[0]):\n",
    "        if classify_instance(tree, dataset.iloc[i]) == dataset.iloc[i][target]:\n",
    "            res = res + 1 \n",
    "    return (res/dataset.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta recorrerá todos los ejemplos del dataset de prueba, y comparará su valor objetivo con el obtenido a partir del arbol de decisión calculado por ID3. El proceso de asignar un valor objetivo a un ejemplo nuevo es llevado a cabo por la función `classify_instance`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_instance(tree, instance):\n",
    "    if isinstance(tree, dict):\n",
    "        feature, branches = next(iter(tree.items()))\n",
    "        feature_value = instance[feature]\n",
    "        if isinstance(branches, dict):\n",
    "            for condition, subtree in branches.items():\n",
    "                if (isEqual(feature_value, condition)):\n",
    "                    return classify_instance(subtree, instance)\n",
    "        else:\n",
    "            return branches\n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando el valor del feature por el que se está iterando con la condición descrita en la posición actual del arbol (mediante la función `isEqual`), la función recorrerá el arbol hasta lograr asignar un valor objetivo a la instancia\\\n",
    "\\\n",
    "Junto con la métrica accuracy, hemos utilizado también el tiempo de ejecución para medir la correctitud de nuestra implementación, y una vez conseguimos una accuracy de alrededor 80% y un tiempo de ejecución razonable (dependiendo del parámetro `max_range_splits`), consideramos nuestra implementación como correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparativa entre preprocesamiento y max_range_splits = 2 con diferentes ratios train/test\n",
    "En el siguiente código, ejecutaremos nuestra implementación de ID3 con el hiperparámetro `max_range_splits = 2` para distintas proporciones de división del dataset entre conjunto de entrenamiento y de prueba. Esto último con el objetivo de evaluar cómo varía la precisión del modelo a medida que cambiamos la proporción de datos destinados al entrenamiento y a la prueba, lo que nos permitirá identificar posibles casos de sobreajuste cuando el modelo se ajusta demasiado a los datos de entrenamiento y no generaliza correctamente a datos nuevos.\\\n",
    "\\\n",
    "Además, para cada división conjuntos entrenamiento-prueba, se ejecutará ID3 tanto para el dataset original como para uno preprocesado, y desplegará el tiempo de ejecución para ambos casos.\\\n",
    "\\\n",
    "Con respecto al preprocesamiento mencionado, lo que realizamos para efectivamente preprocesar el dataset es discretizar todos los atributos continuos previo a la ejecución del algoritmo, para así evitar la discretización en tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 50 % entrenamiento,  50 % test\n",
      "Preprocessed: Accuracy:  82.89719626168224 %  Time:  0:00:05.187340\n",
      "Max Range Split 2: Accuracy:  82.14953271028037 %  Time:  0:00:19.916350\n",
      "\n",
      " 60 % entrenamiento,  40 % test\n",
      "Preprocessed: Accuracy:  82.12616822429906 %  Time:  0:00:05.771565\n",
      "Max Range Split 2: Accuracy:  82.00934579439252 %  Time:  0:00:19.757810\n",
      "\n",
      " 70 % entrenamiento,  30 % test\n",
      "Preprocessed: Accuracy:  81.30841121495327 %  Time:  0:00:06.654187\n",
      "Max Range Split 2: Accuracy:  83.02180685358256 %  Time:  0:00:23.023650\n",
      "\n",
      " 80 % entrenamiento,  20 % test\n",
      "Preprocessed: Accuracy:  80.14018691588785 %  Time:  0:00:07.987066\n",
      "Max Range Split 2: Accuracy:  79.67289719626169 %  Time:  0:00:24.466648\n",
      "\n",
      " 90 % entrenamiento,  10 % test\n",
      "Preprocessed: Accuracy:  79.90654205607477 %  Time:  0:00:08.379983\n",
      "Max Range Split 2: Accuracy:  78.03738317757009 %  Time:  0:00:31.400917\n"
     ]
    }
   ],
   "source": [
    "from id3 import get_splits, split_dataset, id3, split_into_train_test, test_instances, init\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "for i in range(50,100,10):\n",
    "       print('\\n',i,'% entrenamiento, ',100-i, '% test' )\n",
    "       train_ds, test_ds = split_into_train_test(dataset,i/100)\n",
    "       \n",
    "       # Preprocessed\n",
    "       preprocessed_dataset = train_ds.copy()\n",
    "       for cont_feature in continuous_features:\n",
    "              entropy, splits = get_splits(preprocessed_dataset,cont_feature,target,2)\n",
    "              preprocessed_dataset = split_dataset(preprocessed_dataset,cont_feature,splits)\n",
    "       startTime = datetime.now()\n",
    "       preprocessed_decision_tree = id3(preprocessed_dataset, target, features, [], 2, preprocessed_dataset)\n",
    "       preprocessed_time = datetime.now() - startTime\n",
    "       acierto_pre = test_instances(preprocessed_decision_tree,test_ds)\n",
    "       print('Preprocessed: ', end='')\n",
    "       print('Accuracy: ',acierto_pre,'%', ' Time: ', preprocessed_time)\n",
    "       \n",
    "       \n",
    "       # Max_Range_Splits_2\n",
    "       startTime = datetime.now()\n",
    "       max_range_split_2_decision_tree = id3(train_ds,target,features, continuous_features, 2, train_ds)\n",
    "       max_range_split_2_time = datetime.now() - startTime\n",
    "       acierto_run = test_instances(max_range_split_2_decision_tree,test_ds)\n",
    "       print('Max Range Split 2: ', end='')\n",
    "       print('Accuracy: ',acierto_run,'%', ' Time: ', max_range_split_2_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparativa entre preprocesamiento y max_range_splits = 3 con diferentes ratios train/test\n",
    "El objetivo de esta comparativa es análoga a la anterior, solo que cambiando el valor de `max_range_splits` a 3. Por esto último, el tiempo de ejecución de todo el código aumenta drásticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 50 % entrenamiento,  50 % test\n",
      "Preprocessed: Accuracy:  81.77570093457945 %  Time:  0:00:05.543877\n",
      "Max Range Split 3: Accuracy:  84.39252336448598 %  Time:  0:03:08.861896\n",
      "\n",
      " 60 % entrenamiento,  40 % test\n",
      "Preprocessed: Accuracy:  83.41121495327103 %  Time:  0:00:05.461840\n",
      "Max Range Split 3: Accuracy:  80.8411214953271 %  Time:  0:04:23.020943\n",
      "\n",
      " 70 % entrenamiento,  30 % test\n",
      "Preprocessed: Accuracy:  78.19314641744548 %  Time:  0:00:07.355429\n",
      "Max Range Split 3: Accuracy:  81.15264797507788 %  Time:  0:04:35.594333\n",
      "\n",
      " 80 % entrenamiento,  20 % test\n",
      "Preprocessed: Accuracy:  77.57009345794393 %  Time:  0:00:08.072927\n",
      "Max Range Split 3: Accuracy:  78.97196261682244 %  Time:  0:05:01.331624\n",
      "\n",
      " 90 % entrenamiento,  10 % test\n",
      "Preprocessed: Accuracy:  78.03738317757009 %  Time:  0:00:08.293726\n",
      "Max Range Split 3: Accuracy:  75.70093457943925 %  Time:  0:06:02.981560\n"
     ]
    }
   ],
   "source": [
    "from id3 import get_splits, split_dataset, id3, split_into_train_test, test_instances, init\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "for i in range(50,100,10):\n",
    "       print('\\n',i,'% entrenamiento, ',100-i, '% test' )\n",
    "       train_ds, test_ds = split_into_train_test(dataset,i/100)\n",
    "       \n",
    "       # Preprocessed\n",
    "       preprocessed_dataset = train_ds.copy()\n",
    "       for cont_feature in continuous_features:\n",
    "              entropy, splits = get_splits(preprocessed_dataset,cont_feature,target,2)\n",
    "              preprocessed_dataset = split_dataset(preprocessed_dataset,cont_feature,splits)\n",
    "       startTime = datetime.now()\n",
    "       preprocessed_decision_tree = id3(preprocessed_dataset, target, features, [], 2, preprocessed_dataset)\n",
    "       preprocessed_time = datetime.now() - startTime\n",
    "       acierto_pre = test_instances(preprocessed_decision_tree,test_ds)\n",
    "       print('Preprocessed: ', end='')\n",
    "       print('Accuracy: ',acierto_pre,'%', ' Time: ', preprocessed_time)\n",
    "       \n",
    "       # Max_Range_Splits_3\n",
    "       startTime = datetime.now()\n",
    "       max_range_split_3_decision_tree = id3(train_ds,target,features, continuous_features, 3, train_ds)\n",
    "       max_range_split_3_time = datetime.now() - startTime\n",
    "       acierto_run = test_instances(max_range_split_3_decision_tree,test_ds)\n",
    "       print('Max Range Split 3: ', end='')\n",
    "       print('Accuracy: ',acierto_run,'%', ' Time: ', max_range_split_3_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest vs Decision Tree Classifier vs Max_range_splits = 2 vs Max_range_splits = 3\n",
    "Finalmente, ejecutamos los algoritmos Decision Tree Classifier y Random Forest de la librería scikit-learn sobre el conjunto de datos, y comparamos su accuracy con nuestra implementación de ID3.\\\n",
    "\\\n",
    "Brevemente, el algoritmo Random Forest se basa en generar subconjuntos a partir del conjunto original de datos, y para cada uno de estos se calcula un árbol de decisión independiente. Una vez calculados, se promedia los resultados obtenidos para llegar a una mejor predicción final.\\\n",
    "Luego, Decision Tree Classifier es el cálculo de árboles de decisión implementado por scikit-learn.\\\n",
    "\\\n",
    "Cabe destacar que fue necesario preprocesar el dataset para la utilización de estos dos nuevos algoritmos, ya que estos están especificados para datasets de variables continuas.\\\n",
    "Para esto, utilizamos One Hot Encoding, cuya función es la de para cada atributo categórico, crear una columna nueva por cada posible valor que este atributo pueda tomar. De esta manera, elimina los atributos categóricos, y nos permite ejecutar los nuevos algoritmos sobre el nuevo conjunto de datos.\\\n",
    "Observación: Como la gran mayoría de atributos categóricos tienen posible valor 0 o 1, solo tomamos el atributo `trt` para ejecutar One Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy:  89.01869158878505 %\n",
      "Decission Tree Classifier accuracy:  83.41121495327103 %\n",
      "Max Range Split 2 accuracy:  78.50467289719626 %\n",
      "Max Range Split 3 accuracy:  83.41121495327103 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from id3 import init, split_into_train_test, id3, test_instances\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "X = dataset.drop(target, axis=1)\n",
    "y = dataset[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "discrete_features = ['trt']\n",
    "\n",
    "for feat in discrete_features:\n",
    "    ohe.fit(dataset[feat].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    # Transform the training and test data using the fitted OneHotEncoder\n",
    "    # Converts the categorical feature in X_train and X_test to one-hot encoded format\n",
    "    new_train = ohe.transform(X_train[feat].to_numpy().reshape(-1,1))\n",
    "    new_test = ohe.transform(X_test[feat].to_numpy().reshape(-1,1))\n",
    "    \n",
    "    # Create column names for the new one-hot encoded features\n",
    "    column_names = [f\"{feat}_{cat}\" for cat in ohe.categories_[0]]\n",
    "    \n",
    "    for i, col_name in enumerate(column_names):\n",
    "        # Add the new one-hot encoded columns to the X_train and X_test DataFrame\n",
    "        X_train[col_name] = new_train[:, i]\n",
    "        X_test[col_name] = new_test[:, i]\n",
    "    \n",
    "    X_train.drop(feat, axis=1)\n",
    "    X_test.drop(feat, axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=0)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=0)\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_random_forest = random_forest.predict(X_test)\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_random_forest)\n",
    "print('Random forest accuracy: ', accuracy_random_forest*100, '%')\n",
    "\n",
    "y_pred_decission_tree = decision_tree_classifier.predict(X_test)\n",
    "accuracy_decision_tree_classifier = accuracy_score(y_test, y_pred_decission_tree)\n",
    "print('Decission Tree Classifier accuracy: ', accuracy_decision_tree_classifier*100, '%')\n",
    "\n",
    "train_ds, test_ds = split_into_train_test(dataset,0.8)\n",
    "\n",
    "max_range_split_2_decision_tree = id3(train_ds,target,features, continuous_features, 2, train_ds)\n",
    "acierto_split_2 = test_instances(max_range_split_2_decision_tree,test_ds)\n",
    "print('Max Range Split 2 accuracy: ',acierto_split_2,'%')\n",
    "\n",
    "max_range_split_3_decision_tree = id3(train_ds,target,features, continuous_features, 3, train_ds)\n",
    "acierto_split_3 = test_instances(max_range_split_3_decision_tree,test_ds)\n",
    "print('Max Range Split 3 accuracy: ',acierto_split_3,'%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, hemos observado que a medida que se utilzia una mayor porción del conjunto de datos original para el entrenamiento del modelo, disminuye más la accuracy del modelo. Esto es consecuencia del overfitting, que a medida que el modelo es expuesto a una mayor cantidad de datos de entrenamiento, este se sobreajusta a estos datos y se vuelve cada vez peor a la hora de generalizar.\\\n",
    "Esta tendencia la observamos para ambas variaciones del parámetro `max_range_splits`.\\\n",
    "\\\n",
    "No hemos observado una clara tendencia de mayor accuracy en una variación del hiperparámetro sobre otra. Únicamente observamos un mayor tiempo de ejecución para la variación `max_range_splits=3`. Podemos realizar una observación análoga para las instancias que discretizamos el dataset previo a la ejecución del algoritmo, donde la variación más notable fue el tiempo de ejecución\\\n",
    "\\\n",
    "Por último, con respecto a los algoritmos `RandomForest` y `DecisionTreeClassifier`, hemos observado que tienen una mejor accuracy que nuestras implementaciones, y drásticamente menor tiempo de ejecución."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
