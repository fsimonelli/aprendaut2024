{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 1 - Informe\n",
    "\n",
    "### Grupo 4:\n",
    "     - S. Calvo C.I 5.711.417-7     \n",
    "     - X. Iribarnegaray C.I\n",
    "     - J. Simonelli C.I 5.405.358-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este laboratorio es:\n",
    "- Implementar el algoritmo ID3, añadiendo el hiperparámetro *max_range_split*, que determina la cantidad máxima de rangos en los que se puede partir un atributo númerico.\n",
    "- Utilizar scikit-learn para el preprocesamiento de datos y la creación de modelos basados en árboles de decisión.\n",
    "- Evaluar y comparar los modelos generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocesamiento de datos\n",
    "Previo a la ejecución del algoritmo se es provisto a este, mediante una variable global, el conjunto de feature values continuos. Luego, durante la ejecución del algoritmo, se discretizaran los valores asociados a estos feature values.\n",
    "\n",
    "\n",
    "### 2.2 Algoritmo\n",
    "En primer lugar, definimos la función `entropy(dataset, target)` que será utilizada a lo largo de la implementación del algoritmo ID3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(dataset, target):\n",
    "    # value_counts() returns a Series containing the counts of unique values\n",
    "    values = dataset[target].value_counts()\n",
    "    # shape returns the size of dataset, shape[0] being the number of rows\n",
    "    total = dataset.shape[0]\n",
    "    p0 = values.iloc[0]/total\n",
    "    if (len(values) > 1):\n",
    "        p1 = values.iloc[1]/total\n",
    "        return -(p0)*np.log2(p0) - (p1) * np.log2(p1)\n",
    "    else: \n",
    "        return -(p0)*np.log2(p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, el siguiente paso a delinear en el algoritmo es la obtención del \"mejor atributo\". Esto lo logramos mediante la función\\\n",
    "`best_feature(dataset, target, features, continuous_features, max_range_splits)`\\\n",
    "que retorna el siguiente mejor atributo a elegir. Esta decisión es hecha en base a la fórmula de Ganancia vista en el curso, donde, para maximizar la ganancia, basta con minimizar el segundo factor de la fórmula, y, por lo tanto, se eligirá el atributo que minimice este valor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from id3 import actually_split\n",
    "def best_feature(dataset, target, features, continuous_features, max_range_splits):\n",
    "    entropies = []\n",
    "    continuous = {}\n",
    "    for feature in features:\n",
    "        if feature in continuous_features:\n",
    "            # Continuous-Valued feature \n",
    "            aux_entropy, best_split = get_splits(dataset, feature, target, max_range_splits)\n",
    "            entropies.append(aux_entropy)\n",
    "            continuous[feature] = best_split\n",
    "        else :\n",
    "            # Discrete-Valued feature\n",
    "            res = 0\n",
    "            for value, count in dataset[feature].value_counts().items():\n",
    "                res += count*entropy(dataset.loc[dataset[feature] == value], target)\n",
    "            entropies.append(res / dataset.shape[0])\n",
    "    best_feature = features[entropies.index(min(entropies))]\n",
    "    \n",
    "    if not (best_feature in continuous):\n",
    "        return best_feature, dataset\n",
    "    return best_feature, actually_split(dataset.copy(), best_feature, continuous[best_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso en el que el atributo a evaluar sea discreto (`else` del `if`), solamente se lleva a cabo el cálculo descrito previamente.\\\n",
    "Por otro lado, para el caso de evaluar un atributo de valores continuos, se debe realizar otro procedimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dado que por defecto el algoritmo ID3 solamente aplica a ejemplares de valores discretos, en el caso de querer entrenar un modelo mediante un dataset que contiene valores continuos, será necesario discretizarlos en tiempo de ejecución.\\\n",
    "Esto es logrado mediante la división de los valores continuos, con valores calculados con el promedio de dos puntos con valores objetivo distintos. Luego, dependiendo del valor del hiperparámetro *max_range_splits* (de valor 2 o 3 ), utilizamos combinaciones de estos puntos para particionar los valores continuos en rangos. De esta manera, obtenemos finalmente un atributo de valores discretos.\\\n",
    "\\\n",
    "Sin embargo se debe tener en cuenta que, en datasets de gran tamaño, es probable que exista una gran cantidad de posibles puntos de corte, por lo cual es necesario identificar el mejor o mejor par de puntos. De manera similar al cálculo del \"mejor atributo\", se realiza un cálculo de la ganancia para cada split, y se elige el que minimice el segundo factor de la fórmula. Para esto, se utiliza la función \n",
    "`get_splits(dataset, feature, target, max_range_splits)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from id3 import generate_every_pair_from_list\n",
    "from id3 import split_dataset\n",
    "\n",
    "def get_splits(dataset, feature, target, max_range_splits):\n",
    "    min_entropy = 2\n",
    "    dataset = dataset.sort_values(by=feature)\n",
    "    current_target = dataset[target].iloc[0]\n",
    "    dataset_size = dataset.shape[0]\n",
    "    candidate_splits = []\n",
    "    best_values = []\n",
    "    \n",
    "    # Finding splits, iterating through the dataset rows\n",
    "    for i in range(1, dataset_size):\n",
    "        if current_target != dataset[target].iloc[i]:\n",
    "            candidate_splits.append((dataset[feature].iloc[i-1] + dataset[feature].iloc[i])/2)\n",
    "            current_target = dataset[target].iloc[i]\n",
    "\n",
    "    splits = generate_every_pair_from_list(candidate_splits, max_range_splits)\n",
    "    \n",
    "    # Finding the split that minimizes the information gain component \n",
    "    for split in splits:\n",
    "        split_dataset = split_dataset(dataset.copy(), feature, split)\n",
    "        aux_entropy = 0\n",
    "        for value, count in split_dataset[feature].value_counts().items():\n",
    "            aux_entropy += count*entropy(split_dataset.loc[split_dataset[feature] == value], target)\n",
    "        aux_entropy = aux_entropy / split_dataset.shape[0]\n",
    "            \n",
    "        if (aux_entropy < min_entropy):\n",
    "            min_entropy = aux_entropy\n",
    "            best_values = split\n",
    "            \n",
    "    return (min_entropy,best_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por su parte, la función `generate_every_pair_from_list` se encarga de generar todas las posibles combinaciones de puntos de corte para un atributo continuo, y hemos decidido que, en caso de obtener más de 500 combinaciones, se elija un subconjunto aleatorio de tamaño 500 de estas, para evitar un tiempo de ejecución excesivo.\\\n",
    "Además, para alterar el dataset en tiempo de ejecución, se utiliza la función `split_dataset`, que se encarga de discretizar los valores de un atributo continuo en base a el o los puntos de corte obtenidos.\\\n",
    "A través de `get_splits`, obtenemos ya sea el mejor valor o mejor par de valores por los cuales dividir el atributo continuo, junto con el componente de entropía que se utilizará en `best_feature` para determinar el mejor atributo a elegir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, con todas estas funciones implementadas, obtenemos la siguiente implementación del algoritmo ID3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(dataset, target, features, max_range_splits, intact_dataset):\n",
    "    if len(features) == 0 or len(dataset[target].value_counts().index) == 1:\n",
    "        # value_counts[0] is either the only or the most common target value left in the current dataset.\n",
    "        return dataset[target].value_counts().index[0] \n",
    "    best, dataset = best_feature(dataset, target, features, continuous_features, max_range_splits)\n",
    "    decision_tree = {best: {}}\n",
    "    new_features = features.copy()\n",
    "    new_features.remove(best)\n",
    "    if(best in continuous_features):\n",
    "        auxDataset = dataset\n",
    "    else:\n",
    "        auxDataset = intact_dataset\n",
    "    for value in auxDataset[best].value_counts().index:\n",
    "        examples = dataset.loc[dataset[best] == value]\n",
    "        if (len(examples) == 0):\n",
    "            decision_tree[best][value] = auxDataset.value_counts().index[0]\n",
    "        else:\n",
    "            decision_tree[best][value] = id3(examples, target, new_features, max_range_splits, intact_dataset)\n",
    "    return  decision_tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa entre preprocesamiento y runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 50 % entrenamiento,  50 % test\n",
      "Preprocessed: Accuracy:  80.74766355140187 %  Time:  0:00:03.173018\n",
      "Runtime: Accuracy:  82.99065420560747 %  Time:  0:00:12.727490\n",
      "\n",
      " 60 % entrenamiento,  40 % test\n",
      "Preprocessed: Accuracy:  84.11214953271028 %  Time:  0:00:04.868300\n",
      "Runtime: Accuracy:  81.89252336448598 %  Time:  0:00:15.124587\n",
      "\n",
      " 70 % entrenamiento,  30 % test\n",
      "Preprocessed: Accuracy:  78.34890965732087 %  Time:  0:00:06.100208\n",
      "Runtime: Accuracy:  81.93146417445483 %  Time:  0:00:17.222343\n",
      "\n",
      " 80 % entrenamiento,  20 % test\n",
      "Preprocessed: Accuracy:  77.10280373831776 %  Time:  0:00:06.860603\n",
      "Runtime: Accuracy:  82.2429906542056 %  Time:  0:00:18.143070\n",
      "\n",
      " 90 % entrenamiento,  10 % test\n",
      "Preprocessed: Accuracy:  77.10280373831776 %  Time:  0:00:07.572299\n",
      "Runtime: Accuracy:  78.97196261682244 %  Time:  0:00:22.733729\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pprint as pprint\n",
    "from id3 import get_splits, split_dataset, id3, split_into_train_test, test_instances, init\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "tasa_preprocessed_acum = 0\n",
    "preprocessed_time = datetime.now() - datetime.now()\n",
    "\n",
    "tasa_runtime_acum = 0\n",
    "runtime_time = datetime.now() - datetime.now()\n",
    "for i in range(50,100,10):\n",
    "       print('\\n',i,'% entrenamiento, ',100-i, '% test' )\n",
    "       train_ds, test_ds = split_into_train_test(dataset,i/100)\n",
    "       \n",
    "       # Preprocessed\n",
    "       preprocessed_dataset = train_ds.copy()\n",
    "       for cont_feature in continuous_features:\n",
    "              entropy, splits = get_splits(preprocessed_dataset,cont_feature,target,2)\n",
    "              preprocessed_dataset = split_dataset(preprocessed_dataset,cont_feature,splits)\n",
    "       startTime = datetime.now()\n",
    "       preprocessed_decision_tree = id3(preprocessed_dataset, target, features, [], 2, preprocessed_dataset)\n",
    "       preprocessed_time = datetime.now() - startTime\n",
    "       acierto_pre = test_instances(preprocessed_decision_tree,test_ds)\n",
    "       print('Preprocessed: ', end='')\n",
    "       print('Accuracy: ',acierto_pre,'%', ' Time: ', preprocessed_time)\n",
    "       \n",
    "       \n",
    "       # Runtime\n",
    "       startTime = datetime.now()\n",
    "       runtime_decision_tree = id3(train_ds,target,features, continuous_features, 2, train_ds)\n",
    "       runtime_time = datetime.now() - startTime\n",
    "       acierto_run = test_instances(runtime_decision_tree,test_ds)\n",
    "       print('Runtime: ', end='')\n",
    "       print('Accuracy: ',acierto_run,'%', ' Time: ', runtime_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from id3 import init\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "X = dataset.drop(target, axis=1)\n",
    "y = dataset[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "discrete_features = list(set(features) - set(continuous_features))\n",
    "\n",
    "for feat in discrete_features:\n",
    "    ohe.fit(dataset[feat].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    # Transform the training and test data using the fitted OneHotEncoder\n",
    "    # Converts the categorical feature in X_train and X_test to one-hot encoded format\n",
    "    new_train = ohe.transform(X_train[feat].to_numpy().reshape(-1,1))\n",
    "    new_test = ohe.transform(X_test[feat].to_numpy().reshape(-1,1))\n",
    "    \n",
    "    # Create column names for the new one-hot encoded features\n",
    "    column_names = [f\"{feat}_{cat}\" for cat in ohe.categories_[0]]\n",
    "    \n",
    "    for i, col_name in enumerate(column_names):\n",
    "        # Add the new one-hot encoded columns to the X_train and X_test DataFrame\n",
    "        X_train[col_name] = new_train[:, i]\n",
    "        X_test[col_name] = new_test[:, i]\n",
    "\n",
    "print(X_train)\n",
    "    \n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=0)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Random forest accuracy: ', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "X = dataset.drop(target, axis=1)\n",
    "y = dataset[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "discrete_features = list(set(features) - set(continuous_features))\n",
    "\n",
    "for feat in discrete_features:\n",
    "    ohe.fit(dataset[feat].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    # Transform the training and test data using the fitted OneHotEncoder\n",
    "    # Converts the categorical feature in X_train and X_test to one-hot encoded format\n",
    "    new_train = ohe.transform(X_train[feat].to_numpy().reshape(-1,1))\n",
    "    new_test = ohe.transform(X_test[feat].to_numpy().reshape(-1,1))\n",
    "    \n",
    "    # Create column names for the new one-hot encoded features\n",
    "    column_names = [f\"{feat}_{cat}\" for cat in ohe.categories_[0]]\n",
    "    \n",
    "    for i, col_name in enumerate(column_names):\n",
    "        # Add the new one-hot encoded columns to the X_train and X_test DataFrame\n",
    "        X_train[col_name] = new_train[:, i]\n",
    "        X_test[col_name] = new_test[:, i]\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Decision Tree Classifier accuracy: ', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.3 Evaluación\n",
    "Discutir:\n",
    "    - Calculo de porcentaje\n",
    "    - Division en subconjunto de entrenamiento y evaluacion- Qué conjunto de métricas se utilizan para la evaluación de la solución y su definición\n",
    "- Sobre qué conjunto(s) se realiza el entrenamiento, ajuste de la solución, evaluación, etc. Explicar cómo se construyen estos conjuntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dado que por defecto algoritmo ID3 solamente aplica a ejemplares de valores discretos, en el caso de querer entrenar un modelo mediante un dataset que contiene valores continuos, será necesario discretizarlos en tiempo de ejecución. Esto es logrado mediante la división de los valores continuos utilizando *splits*, con posibles valores calculados con el promedio de dos puntos con valores objetivo distintos. Luego, dependiendo del valor del hiperparámetro *max_range_splits* (2 o 3 ), . Sin embargo, en datasets de gran tamaño, es probable que exista una gran cantidad de posibles splits, por lo cual es necesario identificar \n",
    "\n",
    "Cosas a mencionar:\n",
    "    -Max split y discretizacion\n",
    "\n",
    "## 2.3 Evaluación\n",
    "Discutir:\n",
    "    - Calculo de porcentaje\n",
    "    - Division en subconjunto de entrenamiento y evaluacion- Qué conjunto de métricas se utilizan para la evaluación de la solución y su definición\n",
    "- Sobre qué conjunto(s) se realiza el entrenamiento, ajuste de la solución, evaluación, etc. Explicar cómo se construyen estos conjuntos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
