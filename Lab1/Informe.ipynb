{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 1 - Informe\n",
    "\n",
    "### Grupo 4:\n",
    "     - S. Calvo C.I 5.711.417-7     \n",
    "     - X. Iribarnegaray C.I 5.253.705-9\n",
    "     - J. Simonelli C.I 5.405.358-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este laboratorio es:\n",
    "- Implementar el algoritmo ID3, añadiendo el hiperparámetro *max_range_split*, que determina la cantidad máxima de rangos en los que se puede partir un atributo númerico.\n",
    "- Utilizar scikit-learn para el preprocesamiento de datos y la creación de modelos basados en árboles de decisión.\n",
    "- Evaluar y comparar los modelos generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diseño\n",
    "### 2.1 Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(dataset, target, features, continuous_features, max_range_splits, intact_dataset):\n",
    "    if len(features) == 0 or len(dataset[target].value_counts().index) == 1:\n",
    "        # value_counts[0] is either the only or the most common target value left in the current dataset.\n",
    "        return dataset[target].value_counts().index[0] \n",
    " \n",
    "    best, best_splits = best_feature(dataset, target, features, continuous_features, max_range_splits)\n",
    "    decision_tree = {best: {}}\n",
    "    \n",
    "    new_features = features.copy()\n",
    "    new_features.remove(best)\n",
    "    \n",
    "    original_dataset = intact_dataset\n",
    "    \n",
    "    if best_splits:\n",
    "        original_dataset = split_dataset(intact_dataset, best, best_splits)\n",
    "        dataset = split_dataset(dataset, best, best_splits)\n",
    "        \n",
    "    for value in original_dataset[best].value_counts().index:\n",
    "        examples = dataset.loc[dataset[best] == value]\n",
    "        if (len(examples) == 0):\n",
    "            decision_tree[best][value] = original_dataset.loc[original_dataset[best] == value][target].value_counts().index[0]\n",
    "        else:\n",
    "            decision_tree[best][value] = id3(examples, target, new_features, continuous_features, max_range_splits, intact_dataset)\n",
    "    \n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo ID3 es un algoritmo recursivo que, dado un dataset, construye un árbol de decisión de forma recursiva. A continunación describiremos nuestra implementación del algoritmo.\n",
    "\n",
    "#### Casos base:\n",
    "- Si no quedan features (atributos) a analizar, etiquetar la hoja con el valor más común en el dataset restante.\n",
    "- Si todos los ejemplos restantes en el dataset tienen el mismo valor target, etiquetar la hoja con ese valor. \n",
    "\n",
    "En este caso, para ambas condiciones clasificamos la hoja con el valor `dataset[target].value_counts().index[0]`. La función `value_counts()` retorna los valores de la columna target en el dataset, ordenados de más a menos común. Al tomar el primero de estos valores, nos aseguramos de etiquetar la hoja con el más común, para el primer caso, y de etiquetar la hoja con el único valor restante en el dataset para el segundo.\n",
    "\n",
    "#### Recursión:\n",
    "Si no se cumplen las condiciones de los casos base, entonces, el siguiente paso es elegir el mejor atributo: en el caso de nuestra implementación, el atributo que maximiza la ganancia (def. en Teorico). Este se obtiene mediante `best_feature`, que retorna tanto el mejor atributo como, en el caso de que este sea continuo, el mejor o mejores puntos de corte (`best_splits`). Si el atributo es contniuo, es decir, `best_splits` está definido, discretizamos utilizando estos puntos de corte los valores del mejor atributo, tanto en el conjunto de ejemplos restantes como en una copia del dataset_original.\n",
    "\n",
    "Luego, por cada valor posible que puede tomar el mejor atributo (en nuestro caso, los valores que toma este atributo en el dataset original o en la copia discretizada que obtuvimos en el paso anterior), construimos una rama del árbol. En estas ramas, si no hay ejemplos con el mismo valor para el atributo en cuestión, se asigna el valor objetivo más común de los ejemplos del conjunto de datos original, cuyo valor de atributo sea el mismo que el que está siendo evaluado en el momento. De lo contrario, se llama de forma recursiva a la función id3, quitando este atributo del conjunto de atributos a evaluar y usando como nuevo dataset los ejemplos del mismo valor de atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature(dataset, target, features, continuous_features, max_range_splits):\n",
    "    conditional_entropies = []\n",
    "    continuous = {}\n",
    "    for feature in features:\n",
    "        # Continuous-Valued feature \n",
    "        if feature in continuous_features:\n",
    "            aux_conditional_entropy, best_split = get_splits(dataset, feature, target, max_range_splits)\n",
    "            conditional_entropies.append(aux_conditional_entropy)\n",
    "            continuous[feature] = best_split\n",
    "        else :\n",
    "            res = 0\n",
    "            for value, count in dataset[feature].value_counts().items():\n",
    "                res += count*entropy(dataset.loc[dataset[feature] == value], target)\n",
    "            conditional_entropies.append(res / dataset.shape[0])\n",
    "    best_feature = features[conditional_entropies.index(min(conditional_entropies))]\n",
    "    \n",
    "    if not (best_feature in continuous):\n",
    "        return best_feature, None\n",
    "    return best_feature, continuous[best_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función best attribute, calcula, dado un conjunto de ejemplos, el atributo que maximiza la ganancia, como definimos en el teórico o, lo que es equivalente, el que minimiza el sustraendo en la fórmula de esta ganancia.\\\n",
    "\\\n",
    "Si el atributo a evaluar es continuo, obtenemos los posibles puntos de corte para categorizar en rangos los valores de este atributo en el conjunto de ejemplos junto con el valor del sustraendo de la ganancia correspondiente, a través de la función `get_splits(dataset, feature, target, max_range_splits)`. Esta función ordena los valores del atributo en el dataset de menor a mayor, y recorre, en ese orden, la columna target. Cuando el valor de la columna target cambia, por ejemplo de la columna i a la columna i+1, se registra el promedio entre el valor del atributo a evaluar en la columna i y el mismo valor en la columna i+1 como un posible punto de corte. Luego, para reducir el tiempo de ejecución del algoritmo, si la cantidad de posibles puntos de corte es mayor a 50, tomamos 50 al azar, y realizamos todas las combinaciones posibles ordenadas de tamaño hasta max_range_split - 1. \n",
    "\n",
    "En caso contrario, (`else` del `if`), se calcula directamente el valor del sustraendo previamente mencionado.\\\n",
    "\n",
    "\n",
    "mediante la división de los valores continuos con valores calculados mediante el promedio de valor de atributo de dos ejemplos con valores objetivo distintos. Luego, dependiendo del valor del hiperparámetro *max_range_splits* (de valor 2 o 3 ), utilizamos combinaciones de estos puntos para particionar los valores continuos en rangos. De esta manera, obtenemos finalmente un atributo de valores discretos.\\\n",
    "\\\n",
    "Sin embargo se debe tener en cuenta que, en datasets de gran tamaño, es probable que exista una gran cantidad de posibles puntos de corte, por lo cual es necesario identificar el mejor o mejor par de puntos. De manera similar al cálculo del \"mejor atributo\", se realiza un cálculo de la ganancia para cada split, y se elige el que minimice el segundo factor de la fórmula. Para esto, se utiliza la función \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(dataset, feature, target, max_range_splits):\n",
    "    min_conditional_entropy = 2\n",
    "    dataset = dataset.sort_values(by=feature)\n",
    "    current_target = dataset[target].iloc[0]\n",
    "    dataset_size = dataset.shape[0]\n",
    "    candidate_splits = []\n",
    "    best_splits = []\n",
    "    \n",
    "    # Finding splits\n",
    "    for i in range(1, dataset_size):\n",
    "        if current_target != dataset[target].iloc[i]:\n",
    "            candidate_splits.append((dataset[feature].iloc[i-1] + dataset[feature].iloc[i])/2)\n",
    "            current_target = dataset[target].iloc[i]\n",
    "    \n",
    "    sample = candidate_splits\n",
    "    if len(candidate_splits) > 50:\n",
    "        sample = random.sample(candidate_splits, 50)\n",
    "    \n",
    "    splits = generate_combinations(sample, max_range_splits)\n",
    " \n",
    "    for split in splits:\n",
    "        splitted_dataset = split_dataset(dataset, feature, split)\n",
    "        aux_conditional_entropy = 0\n",
    "        for value, count in splitted_dataset[feature].value_counts().items():\n",
    "            aux_conditional_entropy += count*entropy(splitted_dataset.loc[splitted_dataset[feature] == value], target)\n",
    "        aux_conditional_entropy = aux_conditional_entropy / splitted_dataset.shape[0]\n",
    "            \n",
    "        if (aux_conditional_entropy < min_conditional_entropy):\n",
    "            min_conditional_entropy = aux_conditional_entropy\n",
    "            best_splits = split\n",
    "            \n",
    "    return (min_conditional_entropy, best_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por su parte, la función `generate_combinations` genera todas las posibles combinaciones de los pares de splits pasados como parametro `sample` en el caso de que `max_range_splits = 3`. Luego, se ha optado por tomar una muestra aleatoria de 50 elementos (splits singulares o pares) de la lista de splits candidato en el caso de contar con más de 50 de estos, con el fin de reducir el tiempo de ejecución. \\\n",
    "Además, para modificar el dataset en tiempo de ejecución, se utiliza la función `split_dataset`, que se encarga de discretizar los valores de un atributo continuo en base a el o los puntos de corte pasados.\\\n",
    "A través de `get_splits`, obtenemos ya sea el mejor valor o mejor par de valores por los cuales dividir el atributo continuo, junto con el componente de entropía que se utilizará en `best_feature` para determinar el mejor atributo a elegir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(dataset, target):\n",
    "    # value_counts() returns a Series containing the counts of unique values\n",
    "    values = dataset[target].value_counts()\n",
    "    # shape returns the size of dataset, shape[0] being the number of rows\n",
    "    total = dataset.shape[0]\n",
    "    p0 = values.iloc[0]/total\n",
    "    if (len(values) > 1):\n",
    "        p1 = values.iloc[1]/total\n",
    "        return -(p0)*np.log2(p0) - (p1) * np.log2(p1)\n",
    "    else: \n",
    "        return -(p0)*np.log2(p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa entre preprocesamiento y max_range_splits = 2 con diferentes ratios train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código, ejecutaremos nuestra implementación de ID3 con el hiperparámetro `max_range_splits = 2` para distintas proporciones de división del dataset entre conjunto de entrenamiento y de prueba. Esto último con el objetivo de evaluar cómo varía la precisión del modelo a medida que cambiamos la proporción de datos destinados al entrenamiento y a la prueba, lo que nos permitirá identificar posibles casos de sobreajuste cuando el modelo se ajusta demasiado a los datos de entrenamiento y no generaliza correctamente a datos nuevos.\\\n",
    "\\\n",
    "Además, para cada división conjuntos entrenamiento-prueba, se ejecutará ID3 tanto para el dataset original como para uno preprocesado, y desplegará el tiempo de ejecución para ambos casos.\\\n",
    "\\\n",
    "Con respecto al preprocesamiento mencionado, lo que realizamos para efectivamente preprocesar el dataset es discretizar todos los atributos continuos previo a la ejecución del algoritmo, para así evitar la discretización en tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 50 % entrenamiento,  50 % test\n",
      "Preprocessed: Accuracy:  82.33644859813084 %  Time:  0:00:02.892931\n",
      "Max Range Split 2: Accuracy:  83.0841121495327 %  Time:  0:00:11.697056\n",
      "Precision:  65.2014652014652\n",
      "Recall:  67.42424242424242\n",
      "F1 score:  66.29422718808193\n",
      "\n",
      " 60 % entrenamiento,  40 % test\n",
      "Preprocessed: Accuracy:  83.76168224299066 %  Time:  0:00:03.804028\n",
      "Max Range Split 2: Accuracy:  81.07476635514018 %  Time:  0:00:13.082313\n",
      "Precision:  60.48780487804878\n",
      "Recall:  60.48780487804878\n",
      "F1 score:  60.48780487804877\n",
      "\n",
      " 70 % entrenamiento,  30 % test\n",
      "Preprocessed: Accuracy:  79.2834890965732 %  Time:  0:00:04.467416\n",
      "Max Range Split 2: Accuracy:  81.30841121495327 %  Time:  0:00:14.425438\n",
      "Precision:  59.756097560975604\n",
      "Recall:  64.47368421052632\n",
      "F1 score:  62.0253164556962\n",
      "\n",
      " 80 % entrenamiento,  20 % test\n",
      "Preprocessed: Accuracy:  79.67289719626169 %  Time:  0:00:05.823763\n",
      "Max Range Split 2: Accuracy:  78.97196261682244 %  Time:  0:00:17.116764\n",
      "Precision:  56.19047619047619\n",
      "Recall:  57.28155339805825\n",
      "F1 score:  56.73076923076923\n",
      "\n",
      " 90 % entrenamiento,  10 % test\n",
      "Preprocessed: Accuracy:  78.50467289719626 %  Time:  0:00:05.632714\n",
      "Max Range Split 2: Accuracy:  76.16822429906543 %  Time:  0:00:18.239834\n",
      "Precision:  54.09836065573771\n",
      "Recall:  58.92857142857143\n",
      "F1 score:  56.41025641025641\n"
     ]
    }
   ],
   "source": [
    "from id3 import get_splits, split_dataset, id3, split_into_train_test, test_instances, init, calculate_precision, calculate_f1_score, calculate_recall\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "for i in range(50,100,10):\n",
    "       print('\\n',i,'% entrenamiento, ',100-i, '% test' )\n",
    "       train_ds, test_ds = split_into_train_test(dataset,i/100)\n",
    "       \n",
    "       # Preprocessed\n",
    "       preprocessed_dataset = train_ds.copy()\n",
    "       for cont_feature in continuous_features:\n",
    "              entropy, splits = get_splits(preprocessed_dataset,cont_feature,target,2)\n",
    "              preprocessed_dataset = split_dataset(preprocessed_dataset,cont_feature,splits)\n",
    "       startTime = datetime.now()\n",
    "       preprocessed_decision_tree = id3(preprocessed_dataset, target, features, [], 2, preprocessed_dataset)\n",
    "       preprocessed_time = datetime.now() - startTime\n",
    "       acierto_pre = test_instances(preprocessed_decision_tree,test_ds)\n",
    "       print('Preprocessed: ', end='')\n",
    "       print('Accuracy: ',acierto_pre,'%', ' Time: ', preprocessed_time)\n",
    "       \n",
    "       \n",
    "       # Max_Range_Splits_2\n",
    "       startTime = datetime.now()\n",
    "       max_range_split_2_decision_tree = id3(train_ds,target,features, continuous_features, 2, train_ds)\n",
    "       max_range_split_2_time = datetime.now() - startTime\n",
    "       acierto_run = test_instances(max_range_split_2_decision_tree,test_ds)\n",
    "       precision = calculate_precision(max_range_split_2_decision_tree,test_ds,target)\n",
    "       recall = calculate_recall(max_range_split_2_decision_tree,test_ds,target)\n",
    "       f1_score = calculate_f1_score(max_range_split_2_decision_tree,test_ds,target)\n",
    "       print('Max Range Split 2: ', end='')\n",
    "       print('Accuracy: ',acierto_run,'%', ' Time: ', max_range_split_2_time)\n",
    "       print('Precision: ', precision)\n",
    "       print('Recall: ', recall)\n",
    "       print('F1 score: ', f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa entre preprocesamiento y max_range_splits = 3 con diferentes ratios train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta comparativa es análoga a la anterior, solo que cambiando el valor de `max_range_splits` a 3. Por esto último, el tiempo de ejecución de todo el código aumenta drásticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 50 % entrenamiento,  50 % test\n",
      "Preprocessed: Accuracy:  81.49532710280374 %  Time:  0:00:06.951673\n",
      "Max Range Split 3: Accuracy:  83.83177570093457 %  Time:  0:04:42.106825\n",
      "\n",
      " 60 % entrenamiento,  40 % test\n",
      "Preprocessed: Accuracy:  81.42523364485982 %  Time:  0:00:06.229751\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_513727/3542271049.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macierto_pre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' Time: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m        \u001b[0;31m# Max_Range_Splits_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m        \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m        \u001b[0mmax_range_split_3_decision_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinuous_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m        \u001b[0mmax_range_split_3_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m        \u001b[0macierto_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_range_split_3_decision_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Max Range Split 3: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fing/2s2024/aprendaut/aprendaut2024/Lab1/id3.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dataset, target, features, continuous_features, max_range_splits, intact_dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mdecision_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mdecision_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinuous_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_range_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintact_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fing/2s2024/aprendaut/aprendaut2024/Lab1/id3.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dataset, target, features, continuous_features, max_range_splits, intact_dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mdecision_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mdecision_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinuous_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_range_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintact_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fing/2s2024/aprendaut/aprendaut2024/Lab1/id3.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dataset, target, features, continuous_features, max_range_splits, intact_dataset)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# value_counts[0] is either the only or the most common target value left in the current dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinuous_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_range_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mdecision_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fing/2s2024/aprendaut/aprendaut2024/Lab1/id3.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dataset, target, features, continuous_features, max_range_splits)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mcontinuous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Continuous-Valued feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontinuous_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0maux_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_range_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mconditional_entropies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mcontinuous\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fing/2s2024/aprendaut/aprendaut2024/Lab1/id3.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dataset, feature, target, max_range_splits)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0msplitted_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0maux_conditional_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplitted_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0maux_conditional_entropy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplitted_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0maux_conditional_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_conditional_entropy\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msplitted_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;36m4.0\u001b[0m    \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mNaN\u001b[0m    \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0mName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \"\"\"\n\u001b[0;32m-> 1010\u001b[0;31m         return algorithms.value_counts_internal(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                 )\n\u001b[1;32m    949\u001b[0m             \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleArrayManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mManager\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_copy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_mgr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_item_cache\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_attrs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from id3 import get_splits, split_dataset, id3, split_into_train_test, test_instances, init\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "for i in range(50,100,10):\n",
    "       print('\\n',i,'% entrenamiento, ',100-i, '% test' )\n",
    "       train_ds, test_ds = split_into_train_test(dataset,i/100)\n",
    "       \n",
    "       # Preprocessed\n",
    "       preprocessed_dataset = train_ds.copy()\n",
    "       for cont_feature in continuous_features:\n",
    "              entropy, splits = get_splits(preprocessed_dataset,cont_feature,target,2)\n",
    "              preprocessed_dataset = split_dataset(preprocessed_dataset,cont_feature,splits)\n",
    "       startTime = datetime.now()\n",
    "       preprocessed_decision_tree = id3(preprocessed_dataset, target, features, [], 2, preprocessed_dataset)\n",
    "       preprocessed_time = datetime.now() - startTime\n",
    "       acierto_pre = test_instances(preprocessed_decision_tree,test_ds)\n",
    "       print('Preprocessed: ', end='')\n",
    "       print('Accuracy: ',acierto_pre,'%', ' Time: ', preprocessed_time)\n",
    "       \n",
    "       # Max_Range_Splits_3\n",
    "       startTime = datetime.now()\n",
    "       max_range_split_3_decision_tree = id3(train_ds,target,features, continuous_features, 3, train_ds)\n",
    "       max_range_split_3_time = datetime.now() - startTime\n",
    "       acierto_run = test_instances(max_range_split_3_decision_tree,test_ds)\n",
    "       print('Max Range Split 3: ', end='')\n",
    "       print('Accuracy: ',acierto_run,'%', ' Time: ', max_range_split_3_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest vs Decision Tree Classifier vs Max_range_splits = 2 vs Max_range_splits = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy:  88.08411214953271 %\n",
      "Decission Tree Classifier accuracy:  84.34579439252336 %\n",
      "Max Range Split 2 accuracy:  83.64485981308411 %\n",
      "Max Range Split 3 accuracy:  80.8411214953271 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from id3 import init, split_into_train_test, id3, test_instances\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "X = dataset.drop(target, axis=1)\n",
    "y = dataset[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "discrete_features = list(set(features) - set(continuous_features))\n",
    "\n",
    "for feat in discrete_features:\n",
    "    ohe.fit(dataset[feat].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    # Transform the training and test data using the fitted OneHotEncoder\n",
    "    # Converts the categorical feature in X_train and X_test to one-hot encoded format\n",
    "    new_train = ohe.transform(X_train[feat].to_numpy().reshape(-1,1))\n",
    "    new_test = ohe.transform(X_test[feat].to_numpy().reshape(-1,1))\n",
    "    \n",
    "    # Create column names for the new one-hot encoded features\n",
    "    column_names = [f\"{feat}_{cat}\" for cat in ohe.categories_[0]]\n",
    "    \n",
    "    for i, col_name in enumerate(column_names):\n",
    "        # Add the new one-hot encoded columns to the X_train and X_test DataFrame\n",
    "        X_train[col_name] = new_train[:, i]\n",
    "        X_test[col_name] = new_test[:, i]\n",
    "    \n",
    "    X_train.drop(feat, axis=1)\n",
    "    X_test.drop(feat, axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=0)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=0)\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_random_forest = random_forest.predict(X_test)\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_random_forest)\n",
    "print('Random forest accuracy: ', accuracy_random_forest*100, '%')\n",
    "\n",
    "y_pred_decission_tree = decision_tree_classifier.predict(X_test)\n",
    "accuracy_decision_tree_classifier = accuracy_score(y_test, y_pred_decission_tree)\n",
    "print('Decission Tree Classifier accuracy: ', accuracy_decision_tree_classifier*100, '%')\n",
    "\n",
    "train_ds, test_ds = split_into_train_test(dataset,0.8)\n",
    "\n",
    "max_range_split_2_decision_tree = id3(train_ds,target,features, continuous_features, 2, train_ds)\n",
    "acierto_split_2 = test_instances(max_range_split_2_decision_tree,test_ds)\n",
    "print('Max Range Split 2 accuracy: ',acierto_split_2,'%')#, ' Time: ', preprocessed_time)\n",
    "\n",
    "max_range_split_3_decision_tree = id3(train_ds,target,features, continuous_features, 3, train_ds)\n",
    "acierto_split_3 = test_instances(max_range_split_3_decision_tree,test_ds)\n",
    "print('Max Range Split 3 accuracy: ',acierto_split_3,'%')#, ' Time: ', preprocessed_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dataset, features, continuous_features, target = init()\n",
    "\n",
    "X = dataset.drop(target, axis=1)\n",
    "y = dataset[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "discrete_features = list(set(features) - set(continuous_features))\n",
    "\n",
    "for feat in discrete_features:\n",
    "    ohe.fit(dataset[feat].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    # Transform the training and test data using the fitted OneHotEncoder\n",
    "    # Converts the categorical feature in X_train and X_test to one-hot encoded format\n",
    "    new_train = ohe.transform(X_train[feat].to_numpy().reshape(-1,1))\n",
    "    new_test = ohe.transform(X_test[feat].to_numpy().reshape(-1,1))\n",
    "    \n",
    "    # Create column names for the new one-hot encoded features\n",
    "    column_names = [f\"{feat}_{cat}\" for cat in ohe.categories_[0]]\n",
    "    \n",
    "    for i, col_name in enumerate(column_names):\n",
    "        # Add the new one-hot encoded columns to the X_train and X_test DataFrame\n",
    "        X_train[col_name] = new_train[:, i]\n",
    "        X_test[col_name] = new_test[:, i]\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Decision Tree Classifier accuracy: ', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.3 Evaluación\n",
    "Discutir:\n",
    "    - Calculo de porcentaje\n",
    "    - Division en subconjunto de entrenamiento y evaluacion- Qué conjunto de métricas se utilizan para la evaluación de la solución y su definición\n",
    "- Sobre qué conjunto(s) se realiza el entrenamiento, ajuste de la solución, evaluación, etc. Explicar cómo se construyen estos conjuntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3 Evaluación\n",
    "Discutir:\n",
    "    - Calculo de porcentaje\n",
    "    - Division en subconjunto de entrenamiento y evaluacion- Qué conjunto de métricas se utilizan para la evaluación de la solución y su definición\n",
    "- Sobre qué conjunto(s) se realiza el entrenamiento, ajuste de la solución, evaluación, etc. Explicar cómo se construyen estos conjuntos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
